{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 6888070,
          "sourceType": "datasetVersion",
          "datasetId": 3956862
        }
      ],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahatasham5/Breast_Cancer_Prognosis/blob/main/test_eff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"color:yellow;display:inline-block;border-radius:5px;background-color:#007BA7;font-family:Nexa;overflow:hidden\"><p style=\"padding:15px;color:yellow;overflow:hidden;font-size:100%;letter-spacing:0.5px;margin:0\"><b> </b> Import Modules</p></div>"
      ],
      "metadata": {
        "id": "2fUBVSQuAFjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WNN-zDNF-dhz",
        "outputId": "f0e8ab70-4558-4a45-bed0-280124ed59e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=cae08d73471746fb676f6deaab96eb0627dcb05b2266943e77dc5168b90fd789\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "metadata": {
        "id": "gsnbRDhM7prX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torchvision.models.densenet import densenet121"
      ],
      "metadata": {
        "id": "aZ8hZTB-7uJB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained ResNet model\n",
        "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "# Freeze all the parameters in the model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "num_ftrs = model._fc.in_features\n",
        "model._fc = nn.Linear(num_ftrs,num_classes)\n",
        "\n",
        "# Check if a GPU is available and move the model to GPU if it is\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psY4t0GqA22r",
        "outputId": "fc275121-2d33-407e-a3ea-fb1f4f76e9fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcXWlLLzTHlM",
        "outputId": "e4a1c48e-789b-4bef-ef99-1143c208a3e5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSC-T1K1CemS",
        "outputId": "b700663b-f72b-41b8-e755-b037ae4052f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CSE/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uP28BdoUDG7",
        "outputId": "cc4bc743-698c-4f62-cc0e-ac35fed0d92a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMMA-DbmCmuh",
        "outputId": "e90306c3-7fda-43dd-a2a8-04eb228cae71"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m'Biglycan breast cancer dataset'\u001b[0m/   model_checkpoint_alexnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 for ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalization for ResNet\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "dataset = datasets.ImageFolder(\"Biglycan breast cancer dataset\", transform=transform)\n",
        "\n",
        "# Splitting dataset into train and validation\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "b7btuRwA77qR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKYtD_LwUGIW",
        "outputId": "70e0ed13-d6cf-4d7f-f2de-0edcc73957a8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m'Biglycan breast cancer dataset'\u001b[0m/   model_checkpoint_alexnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model._fc.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "wXKGtWWQ8eLP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emfJsA6I8_7V",
        "outputId": "717d375f-02d1-4439-ebfa-f3529d56d4b8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m'Biglycan breast cancer dataset'\u001b[0m/   model_checkpoint_alexnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the checkpoint file path\n",
        "checkpoint_path = 'model_checkpoint_eff.pth'\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).type_as(outputs))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Save the checkpoint after each epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': running_loss / len(train_loader)\n",
        "    }, checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiS5UQJ186CJ",
        "outputId": "e962e16b-2753-49e8-9802-7a93f58cba14"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6770138608084785\n",
            "Epoch 2, Loss: 0.61163862546285\n",
            "Epoch 3, Loss: 0.5750115778711107\n",
            "Epoch 4, Loss: 0.5190998580720689\n",
            "Epoch 5, Loss: 0.5049935579299927\n",
            "Epoch 6, Loss: 0.47200408909055924\n",
            "Epoch 7, Loss: 0.4296405414740245\n",
            "Epoch 8, Loss: 0.40813708636495805\n",
            "Epoch 9, Loss: 0.409041659699546\n",
            "Epoch 10, Loss: 0.40456385413805646\n",
            "Epoch 11, Loss: 0.3884320557117462\n",
            "Epoch 12, Loss: 0.3684675561057197\n",
            "Epoch 13, Loss: 0.37548743685086566\n",
            "Epoch 14, Loss: 0.3549472590287526\n",
            "Epoch 15, Loss: 0.32707030243343777\n",
            "Epoch 16, Loss: 0.3489957054456075\n",
            "Epoch 17, Loss: 0.33425183759795296\n",
            "Epoch 18, Loss: 0.3285731491115358\n",
            "Epoch 19, Loss: 0.31714941561222076\n",
            "Epoch 20, Loss: 0.30839595529768205\n",
            "Epoch 21, Loss: 0.32184340556462604\n",
            "Epoch 22, Loss: 0.3102657629383935\n",
            "Epoch 23, Loss: 0.30016040802001953\n",
            "Epoch 24, Loss: 0.27517767747243244\n",
            "Epoch 25, Loss: 0.2868136233753628\n",
            "Epoch 26, Loss: 0.26063866251044804\n",
            "Epoch 27, Loss: 0.2649291985564762\n",
            "Epoch 28, Loss: 0.2450000395377477\n",
            "Epoch 29, Loss: 0.2612433698442247\n",
            "Epoch 30, Loss: 0.2591349068615172\n",
            "Epoch 31, Loss: 0.2511635919411977\n",
            "Epoch 32, Loss: 0.2622070097261005\n",
            "Epoch 33, Loss: 0.25786322851975757\n",
            "Epoch 34, Loss: 0.2348160727156533\n",
            "Epoch 35, Loss: 0.25145457850562203\n",
            "Epoch 36, Loss: 0.2563128156794442\n",
            "Epoch 37, Loss: 0.24012240767478943\n",
            "Epoch 38, Loss: 0.24091167747974396\n",
            "Epoch 39, Loss: 0.2304101586341858\n",
            "Epoch 40, Loss: 0.2246069461107254\n",
            "Epoch 41, Loss: 0.243120605746905\n",
            "Epoch 42, Loss: 0.2203767779800627\n",
            "Epoch 43, Loss: 0.2283894701136483\n",
            "Epoch 44, Loss: 0.2243190747168329\n",
            "Epoch 45, Loss: 0.23661545746856266\n",
            "Epoch 46, Loss: 0.1990871247318056\n",
            "Epoch 47, Loss: 0.21440382964081234\n",
            "Epoch 48, Loss: 0.20265908539295197\n",
            "Epoch 49, Loss: 0.22000253366099465\n",
            "Epoch 50, Loss: 0.2115001529455185\n",
            "Epoch 51, Loss: 0.18506170809268951\n",
            "Epoch 52, Loss: 0.22808467265632418\n",
            "Epoch 53, Loss: 0.2095669060945511\n",
            "Epoch 54, Loss: 0.2087443553739124\n",
            "Epoch 55, Loss: 0.17961866822507647\n",
            "Epoch 56, Loss: 0.20347625099950367\n",
            "Epoch 57, Loss: 0.22429688274860382\n",
            "Epoch 58, Loss: 0.2054190287987391\n",
            "Epoch 59, Loss: 0.20449381404452854\n",
            "Epoch 60, Loss: 0.1839679562383228\n",
            "Epoch 61, Loss: 0.17711137069596183\n",
            "Epoch 62, Loss: 0.20804904732439253\n",
            "Epoch 63, Loss: 0.1546336536606153\n",
            "Epoch 64, Loss: 0.18319081101152632\n",
            "Epoch 65, Loss: 0.18190120822853512\n",
            "Epoch 66, Loss: 0.1722567139400376\n",
            "Epoch 67, Loss: 0.21440038250552285\n",
            "Epoch 68, Loss: 0.20756717771291733\n",
            "Epoch 69, Loss: 0.17178012430667877\n",
            "Epoch 70, Loss: 0.16484638883007896\n",
            "Epoch 71, Loss: 0.21164037618372175\n",
            "Epoch 72, Loss: 0.21121678253014883\n",
            "Epoch 73, Loss: 0.17339646402332518\n",
            "Epoch 74, Loss: 0.16975770973496968\n",
            "Epoch 75, Loss: 0.17847713165813023\n",
            "Epoch 76, Loss: 0.1437470664580663\n",
            "Epoch 77, Loss: 0.2077305076850785\n",
            "Epoch 78, Loss: 0.14263322369919884\n",
            "Epoch 79, Loss: 0.16735693564017615\n",
            "Epoch 80, Loss: 0.1502724571360482\n",
            "Epoch 81, Loss: 0.16410082827011743\n",
            "Epoch 82, Loss: 0.16842933578623664\n",
            "Epoch 83, Loss: 0.14442255761888292\n",
            "Epoch 84, Loss: 0.18411712100108465\n",
            "Epoch 85, Loss: 0.14295481145381927\n",
            "Epoch 86, Loss: 0.1686562556359503\n",
            "Epoch 87, Loss: 0.18421076900429195\n",
            "Epoch 88, Loss: 0.13698005345132616\n",
            "Epoch 89, Loss: 0.14832933909363216\n",
            "Epoch 90, Loss: 0.17655041896634632\n",
            "Epoch 91, Loss: 0.16064914729860094\n",
            "Epoch 92, Loss: 0.1449987209505505\n",
            "Epoch 93, Loss: 0.14670153458913168\n",
            "Epoch 94, Loss: 0.13823854592111376\n",
            "Epoch 95, Loss: 0.14672867043150795\n",
            "Epoch 96, Loss: 0.14939233991834852\n",
            "Epoch 97, Loss: 0.15874039547310936\n",
            "Epoch 98, Loss: 0.16805829521682528\n",
            "Epoch 99, Loss: 0.14594795720444786\n",
            "Epoch 100, Loss: 0.1583715420630243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo77SsrU-ITI",
        "outputId": "d4c0bfab-7bf0-498f-8b34-95378fd1eb59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m'Biglycan breast cancer dataset'\u001b[0m/   model_checkpoint_alexnet.pth   model_checkpoint_eff.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the checkpoint file path\n",
        "checkpoint_path = 'model_checkpoint_eff.pth'\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "print(f\"Resuming training from epoch {start_epoch}, with loss: {loss}\")\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).type_as(outputs))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Save the checkpoint after each epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': running_loss / len(train_loader)\n",
        "    }, checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYFAWNVz-UCM",
        "outputId": "fbe79368-91c2-4902-93ce-2996f26d7b56"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming training from epoch 100, with loss: 0.1583715420630243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the saved model weights\n",
        "model.load_state_dict(torch.load('model_.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "crBfjrJnMy-V",
        "outputId": "df28048a-d702-43de-ef05-26191f2120d4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'model_.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-774343a2b19a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the saved model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load('model_checkpoint_eff.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Optionally, you can also load the optimizer state and other saved data\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']"
      ],
      "metadata": {
        "id": "eIhnmffoM5qJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set model to evaluation mode\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate and print the evaluation metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(all_targets, np.array(all_preds).flatten()))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_targets, np.array(all_preds).flatten(), target_names=['Healthy', 'Cancerous']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GCHoGdJTSnC",
        "outputId": "dd2cc049-7a20-46d9-f190-b95f0d68b416"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[37  4]\n",
            " [ 2 25]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Healthy       0.95      0.90      0.92        41\n",
            "   Cancerous       0.86      0.93      0.89        27\n",
            "\n",
            "    accuracy                           0.91        68\n",
            "   macro avg       0.91      0.91      0.91        68\n",
            "weighted avg       0.91      0.91      0.91        68\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Apply sigmoid since we used BCEWithLogitsLoss\n",
        "        predicted_probs = torch.sigmoid(outputs)\n",
        "\n",
        "        # Convert probabilities to predicted class (0 or 1)\n",
        "        predicted = predicted_probs > 0.5\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.flatten() == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3YUQjo0TqDf",
        "outputId": "470012b9-d17c-4bbd-e76f-a9992b255f27"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 91.18%\n"
          ]
        }
      ]
    }
  ]
}